{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75147ad",
   "metadata": {},
   "source": [
    "# Universal Bank -WE04 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b5e867f",
   "metadata": {},
   "source": [
    "1.0 Import and install python libraries we require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3137ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d0c1c",
   "metadata": {},
   "source": [
    "# 2.0 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a33445",
   "metadata": {},
   "outputs": [],
   "source": [
    "univBank = pd.read_csv(r\"C:\\DSP\\UniversalBank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91141db1",
   "metadata": {},
   "source": [
    "Process the data by checking the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c68d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "Age                   0\n",
       "Experience            0\n",
       "Income                0\n",
       "ZIP Code              0\n",
       "Family                0\n",
       "CCAvg                 0\n",
       "Education             0\n",
       "Mortgage              0\n",
       "Personal Loan         0\n",
       "Securities Account    0\n",
       "CD Account            0\n",
       "Online                0\n",
       "CreditCard            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "univBank.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cd84fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "univBank.drop(univBank.columns[[0,4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b2931",
   "metadata": {},
   "source": [
    "split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d7bb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into validation and training set\n",
    "train_df, test_df = train_test_split(univBank, test_size=0.3)\n",
    "\n",
    "# to reduce repetition in later code, create variables to represent the columns\n",
    "# that are our predictors and target\n",
    "target = 'Securities Account'\n",
    "predictors = list(univBank.columns)\n",
    "predictors.remove(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99fda425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a standard scaler and fit it to the training set of predictors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "cols_to_stdize = predictors               \n",
    "               \n",
    "# Transform the predictors of training and validation sets\n",
    "train_df[cols_to_stdize] = scaler.fit_transform(train_df[cols_to_stdize]) # train_predictors is not a numpy array\n",
    "test_df[cols_to_stdize] = scaler.transform(test_df[cols_to_stdize]) # validation_target is now a series object\n",
    "\n",
    "train_X = train_df[predictors]\n",
    "train_y = train_df[target] # train_target is now a series objecttrain_df.to_csv('airbnb_train_df.csv', index=False)\n",
    "test_X = train_df[predictors]\n",
    "test_y = test_df[target] # validation_target is now a series object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58938496",
   "metadata": {},
   "source": [
    "# Logistic Regression using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a95069a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.6730434782608696\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 612, 'C': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "1005 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "350 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "280 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "375 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.61631373 0.67272727        nan        nan 0.61602564\n",
      " 0.         0.61631373 0.38533333 0.                nan        nan\n",
      " 0.61631373        nan        nan        nan 0.38533333 0.\n",
      "        nan 0.63061472        nan 0.38533333 0.63061472 0.61631373\n",
      " 0.62088889        nan        nan 0.38533333        nan        nan\n",
      "        nan        nan 0.67304348 0.61631373 0.61631373 0.\n",
      " 0.         0.62088889 0.61602564        nan 0.62088889 0.62088889\n",
      " 0.         0.         0.61631373        nan 0.61602564        nan\n",
      " 0.         0.61602564 0.64761905 0.                nan 0.\n",
      " 0.67142857 0.63061472 0.61602564        nan        nan        nan\n",
      " 0.61602564        nan 0.61631373 0.61602564        nan        nan\n",
      " 0.                nan 0.         0.62088889        nan 0.62088889\n",
      " 0.62088889 0.62088889        nan 0.         0.         0.61602564\n",
      " 0.                nan 0.67304348 0.63061472 0.         0.61631373\n",
      "        nan        nan 0.62088889 0.67142857 0.63061472 0.62088889\n",
      " 0.61631373 0.                nan 0.62088889 0.62088889        nan\n",
      " 0.                nan 0.66859903 0.         0.62088889        nan\n",
      "        nan 0.62088889        nan 0.         0.62088889        nan\n",
      "        nan        nan        nan        nan 0.63061472 0.64761905\n",
      " 0.61602564 0.62088889 0.61631373        nan 0.62088889 0.62088889\n",
      " 0.61602564 0.                nan        nan 0.                nan\n",
      "        nan 0.67304348 0.                nan 0.61631373 0.61602564\n",
      "        nan        nan        nan 0.38533333 0.63061472        nan\n",
      "        nan 0.         0.                nan        nan        nan\n",
      " 0.61602564 0.         0.66859903 0.63061472 0.61602564        nan\n",
      " 0.62088889        nan        nan 0.         0.62088889        nan\n",
      " 0.38533333        nan        nan        nan 0.61631373 0.62088889\n",
      "        nan        nan 0.61631373        nan 0.66859903 0.61631373\n",
      "        nan 0.64761905 0.         0.61602564 0.         0.63061472\n",
      " 0.61631373        nan 0.         0.61631373 0.66859903 0.62088889\n",
      "        nan 0.67142857 0.38533333        nan 0.62088889 0.62088889\n",
      " 0.61631373 0.38533333        nan 0.         0.63061472 0.\n",
      "        nan 0.64761905        nan 0.         0.62088889        nan\n",
      " 0.61631373 0.61631373 0.62088889 0.61631373        nan        nan\n",
      "        nan 0.67304348 0.67272727 0.61631373        nan 0.62088889\n",
      "        nan 0.                nan        nan 0.62088889 0.67304348\n",
      " 0.61602564        nan        nan        nan        nan 0.61602564\n",
      "        nan 0.         0.         0.                nan 0.\n",
      "        nan 0.62088889 0.61602564 0.62088889 0.62088889        nan\n",
      "        nan 0.61631373 0.63061472 0.61631373        nan        nan\n",
      "        nan 0.62088889 0.61602564        nan 0.61602564 0.61602564\n",
      " 0.61631373        nan        nan        nan 0.38533333        nan\n",
      "        nan        nan 0.                nan        nan 0.\n",
      "        nan        nan 0.62088889        nan 0.62088889        nan\n",
      "        nan 0.66859903        nan        nan        nan 0.64761905\n",
      " 0.62088889        nan        nan 0.66859903 0.64761905        nan\n",
      "        nan        nan        nan        nan 0.66859903        nan\n",
      "        nan 0.61602564        nan 0.61602564 0.67304348        nan\n",
      " 0.61631373 0.61631373 0.         0.61631373        nan        nan\n",
      " 0.62088889        nan        nan        nan 0.38533333 0.61631373\n",
      "        nan 0.67304348 0.67304348 0.67304348        nan        nan\n",
      " 0.         0.61631373        nan        nan        nan        nan\n",
      " 0.67272727 0.61631373 0.38533333 0.61631373 0.64761905 0.61631373\n",
      " 0.61631373        nan        nan 0.62088889 0.62088889        nan\n",
      " 0.         0.                nan        nan 0.67304348        nan\n",
      " 0.67272727 0.62088889 0.61602564        nan 0.62088889 0.38533333\n",
      "        nan 0.61631373 0.                nan 0.62088889 0.61602564\n",
      " 0.62088889        nan        nan        nan 0.67142857        nan\n",
      " 0.61602564 0.38533333        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.67304348 0.62088889\n",
      " 0.67304348 0.67142857 0.67142857 0.62088889        nan        nan\n",
      " 0.61602564        nan        nan 0.61631373 0.66859903 0.63061472\n",
      " 0.                nan 0.61631373 0.38533333        nan 0.67272727\n",
      "        nan 0.67272727 0.61602564 0.62088889 0.63061472        nan\n",
      " 0.62088889 0.38533333 0.64761905        nan 0.                nan\n",
      " 0.         0.67142857        nan 0.         0.         0.67272727\n",
      " 0.61631373 0.62088889 0.61631373 0.         0.63061472 0.66859903\n",
      " 0.61631373        nan 0.38533333        nan 0.         0.61631373\n",
      " 0.63061472 0.62088889        nan 0.66859903 0.61631373 0.67304348\n",
      " 0.67304348 0.61631373        nan        nan 0.62088889        nan\n",
      "        nan        nan 0.                nan 0.67142857 0.62088889\n",
      " 0.67142857        nan        nan 0.38533333 0.61602564 0.\n",
      " 0.         0.64761905 0.         0.62088889 0.62088889 0.\n",
      "        nan        nan 0.61602564 0.62088889 0.         0.62088889\n",
      " 0.62088889 0.62088889 0.         0.38533333 0.61602564 0.\n",
      " 0.67142857 0.                nan 0.61602564 0.61631373 0.\n",
      " 0.67142857        nan        nan 0.62088889 0.67272727        nan\n",
      "        nan 0.62088889        nan 0.62088889 0.67142857        nan\n",
      "        nan 0.62088889 0.67272727        nan 0.61602564 0.\n",
      " 0.61631373 0.63061472 0.61631373        nan        nan 0.\n",
      "        nan        nan 0.61631373        nan 0.64761905 0.61631373\n",
      "        nan        nan        nan 0.61602564        nan        nan\n",
      " 0.61631373 0.62088889        nan 0.61631373        nan 0.62088889\n",
      "        nan 0.62088889]\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sravani\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.62899733 0.95              nan        nan 0.63233279\n",
      " 0.         0.62899733 0.46194969 0.                nan        nan\n",
      " 0.6374676         nan        nan        nan 0.46194969 0.\n",
      "        nan 0.64140724        nan 0.46194969 0.64140724 0.6374676\n",
      " 0.62735687        nan        nan 0.46194969        nan        nan\n",
      "        nan        nan 0.68772432 0.63375602 0.63375602 0.\n",
      " 0.         0.62735687 0.63056703        nan 0.62735687 0.62735687\n",
      " 0.         0.         0.62899733        nan 0.63056703        nan\n",
      " 0.         0.63233279 0.64916168 0.                nan 0.\n",
      " 0.73852241 0.64140724 0.63233279        nan        nan        nan\n",
      " 0.63056703        nan 0.62899733 0.63056703        nan        nan\n",
      " 0.                nan 0.         0.62735687        nan 0.62735687\n",
      " 0.62735687 0.62735687        nan 0.         0.         0.63233279\n",
      " 0.                nan 0.68772432 0.64140724 0.         0.62899733\n",
      "        nan        nan 0.62735687 0.73852241 0.64140724 0.62735687\n",
      " 0.62899733 0.                nan 0.62735687 0.62735687        nan\n",
      " 0.                nan 0.6905617  0.         0.62735687        nan\n",
      "        nan 0.62735687        nan 0.         0.62735687        nan\n",
      "        nan        nan        nan        nan 0.64140724 0.64916168\n",
      " 0.63233279 0.62735687 0.62899733        nan 0.62735687 0.62735687\n",
      " 0.63056703 0.                nan        nan 0.                nan\n",
      "        nan 0.68772432 0.                nan 0.63375602 0.63233279\n",
      "        nan        nan        nan 0.46194969 0.64140724        nan\n",
      "        nan 0.         0.                nan        nan        nan\n",
      " 0.63233279 0.         0.6905617  0.64140724 0.63056703        nan\n",
      " 0.62735687        nan        nan 0.         0.62735687        nan\n",
      " 0.46194969        nan        nan        nan 0.62899733 0.62735687\n",
      "        nan        nan 0.62899733        nan 0.6905617  0.6374676\n",
      "        nan 0.64916168 0.         0.63056703 0.         0.64140724\n",
      " 0.6374676         nan 0.         0.63375602 0.6905617  0.62735687\n",
      "        nan 0.73852241 0.46194969        nan 0.62735687 0.62735687\n",
      " 0.6374676  0.46194969        nan 0.         0.64140724 0.\n",
      "        nan 0.64916168        nan 0.         0.62735687        nan\n",
      " 0.6374676  0.6374676  0.62735687 0.62899733        nan        nan\n",
      "        nan 0.68772432 0.95       0.63375602        nan 0.62735687\n",
      "        nan 0.                nan        nan 0.62735687 0.68772432\n",
      " 0.63233279        nan        nan        nan        nan 0.6332337\n",
      "        nan 0.         0.         0.                nan 0.\n",
      "        nan 0.62735687 0.63056703 0.62735687 0.62735687        nan\n",
      "        nan 0.62899733 0.64140724 0.62899733        nan        nan\n",
      "        nan 0.62735687 0.63233279        nan 0.63233279 0.63056703\n",
      " 0.62899733        nan        nan        nan 0.46194969        nan\n",
      "        nan        nan 0.                nan        nan 0.\n",
      "        nan        nan 0.62735687        nan 0.62735687        nan\n",
      "        nan 0.6905617         nan        nan        nan 0.64916168\n",
      " 0.62735687        nan        nan 0.6905617  0.64916168        nan\n",
      "        nan        nan        nan        nan 0.6905617         nan\n",
      "        nan 0.63233279        nan 0.63056703 0.68772432        nan\n",
      " 0.6374676  0.6374676  0.         0.6374676         nan        nan\n",
      " 0.62735687        nan        nan        nan 0.46194969 0.6374676\n",
      "        nan 0.68772432 0.68772432 0.68772432        nan        nan\n",
      " 0.         0.6374676         nan        nan        nan        nan\n",
      " 0.95       0.62899733 0.46194969 0.62899733 0.64916168 0.62899733\n",
      " 0.6374676         nan        nan 0.62735687 0.62735687        nan\n",
      " 0.         0.                nan        nan 0.68772432        nan\n",
      " 0.95       0.62735687 0.63056703        nan 0.62735687 0.46194969\n",
      "        nan 0.63375602 0.                nan 0.62735687 0.63233279\n",
      " 0.62735687        nan        nan        nan 0.73852241        nan\n",
      " 0.63056703 0.46194969        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.68772432 0.62735687\n",
      " 0.68772432 0.73852241 0.73852241 0.62735687        nan        nan\n",
      " 0.63233279        nan        nan 0.62899733 0.6905617  0.64140724\n",
      " 0.                nan 0.63375602 0.46194969        nan 0.95\n",
      "        nan 0.95       0.63233279 0.62735687 0.64140724        nan\n",
      " 0.62735687 0.46194969 0.64690179        nan 0.                nan\n",
      " 0.         0.73852241        nan 0.         0.         0.95\n",
      " 0.63375602 0.62735687 0.62899733 0.         0.64140724 0.6905617\n",
      " 0.62899733        nan 0.46194969        nan 0.         0.63375602\n",
      " 0.64140724 0.62735687        nan 0.6905617  0.6374676  0.68772432\n",
      " 0.68772432 0.62899733        nan        nan 0.62735687        nan\n",
      "        nan        nan 0.                nan 0.73852241 0.62735687\n",
      " 0.73852241        nan        nan 0.46194969 0.63056703 0.\n",
      " 0.         0.64916168 0.         0.62735687 0.62735687 0.\n",
      "        nan        nan 0.63056703 0.62735687 0.         0.62735687\n",
      " 0.62735687 0.62735687 0.         0.46194969 0.63233279 0.\n",
      " 0.73852241 0.                nan 0.63233279 0.62899733 0.\n",
      " 0.73852241        nan        nan 0.62735687 0.95              nan\n",
      "        nan 0.62735687        nan 0.62735687 0.73852241        nan\n",
      "        nan 0.62735687 0.95              nan 0.63233279 0.\n",
      " 0.62899733 0.64140724 0.62899733        nan        nan 0.\n",
      "        nan        nan 0.6374676         nan 0.64916168 0.63375602\n",
      "        nan        nan        nan 0.63056703        nan        nan\n",
      " 0.62899733 0.62735687        nan 0.62899733        nan 0.62735687\n",
      "        nan 0.62735687]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lreg_rs = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lreg_rs, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c327c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best recall score is 0.6708245243128964\n",
      "... with parameters: {'C': 0.09999999999999998, 'max_iter': 490, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "4000 fits failed out of a total of 8000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'C' parameter of LogisticRegression must be a float in the range (0, inf]. Got -0.9 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.67082452 0.67082452 0.67082452]\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan        nan        nan ... 0.66892857 0.66892857 0.66892857]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "lreg_gs =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = lreg_gs, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit()\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlogreg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989158",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "#Random search\n",
    "score_measure = \"precision\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "SVM_R_out = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = SVM_R_out, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestSVMrandom = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3a10236",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'poly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29552\\922073488.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscore_measure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"recall\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbest_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'poly'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mbest_gamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmin_regulization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'30.1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'poly'"
     ]
    }
   ],
   "source": [
    "#grid search \n",
    "score_measure = \"precision\"\n",
    "kfolds = 3\n",
    "best_kernel = rand_search.best_params_['poly']\n",
    "best_gamma = rand_search.best_params_['gamma']\n",
    "min_regulization=rand_search.best_params_['30.1']\n",
    "best_degree = rand_search.best_params_['4']\n",
    "best_coef0=rand_search.best_params_['6']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization-3,min_regulization+3), \n",
    "               'kernel':[best_kernel],\n",
    "              'gamma':[best_gamma],\n",
    "              'degree': np.arange(best_degree-1,best_degree+1),\n",
    "            'coef0': np.arange(best_coef0-3,best_coef0+3)\n",
    "}\n",
    "\n",
    "SVM_G_out = SVC()\n",
    "grid_search = GridSearchCV(estimator = SVM_G_out, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestSVMgrid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56625012",
   "metadata": {},
   "source": [
    "#  Decision Trees regressor using RandomSearch and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeb7cdad",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29552\\2297243613.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                            return_train_score=True)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The best {score_measure} score is {rand_search.best_score_}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    872\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1769\u001b[0m             ParameterSampler(\n\u001b[0;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmore_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m                 \u001b[0mcandidate_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[0;32m    311\u001b[0m                 \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample_without_replacement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\utils\\_random.pyx\u001b[0m in \u001b[0;36msklearn.utils._random.sample_without_replacement\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,200),  \n",
    "    'min_samples_leaf': np.arange(1,200),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 200), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['squared_error', 'poisson', 'absolute_error', 'friedman_mse'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeRegressor()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4c0924",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'min_samples_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29552\\689819834.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscore_measure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"recall\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mkfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmin_samples_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_samples_split'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_samples_leaf'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmin_impurity_decrease\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'min_impurity_decrease'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'min_samples_split'"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "min_samples_split = rand_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = rand_search.best_params_['min_samples_leaf']\n",
    "min_impurity_decrease = rand_search.best_params_['min_impurity_decrease']\n",
    "max_leaf_nodes = rand_search.best_params_['max_leaf_nodes']\n",
    "max_depth = rand_search.best_params_['max_depth']\n",
    "criterion = rand_search.best_params_['criterion']\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(min_samples_split-2,min_samples_split+2),  \n",
    "    'min_samples_leaf': np.arange(min_samples_leaf-2,min_samples_leaf+2),\n",
    "    'min_impurity_decrease': np.arange(min_impurity_decrease-0.0001, min_impurity_decrease+0.0001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(max_leaf_nodes-2,max_leaf_nodes+2), \n",
    "    'max_depth': np.arange(max_depth-2,max_depth+2), \n",
    "    'criterion': [criterion]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeRegressor()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbff7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 1.0\n",
      "... with parameters: {'min_samples_split': 38, 'min_samples_leaf': 28, 'min_impurity_decrease': 0.0071, 'max_leaf_nodes': 29, 'max_depth': 8, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "#decision tree classifier\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,42),  \n",
    "    'min_samples_leaf': np.arange(1,42),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 42), \n",
    "    'max_depth': np.arange(1,42), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc93f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "The best precision score is 0.923391812865497\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 40, 'max_leaf_nodes': 37, 'min_impurity_decrease': 0.0009, 'min_samples_leaf': 3, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(3,6),  \n",
    "    'min_samples_leaf': np.arange(3,6),\n",
    "    'min_impurity_decrease': np.arange(0.0009, 0.0012,0.0001),\n",
    "    'max_leaf_nodes': np.arange(36,40), \n",
    "    'max_depth': np.arange(39,42), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e67f2",
   "metadata": {},
   "source": [
    "As per the results among the above models according to precision score polynomial kernal model with grid search stands out best with  and second best goes to Polynomial kernal SVM classification model using random search  So finally polynomial kernal model with grid search fits best followed by Polynomial kernal SVM classification model using random searchwhen compared to others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
